
% Default to the notebook output style

    


% Inherit from the specified cell style.




    
\documentclass[11pt]{article}

    
    
    \usepackage[T1]{fontenc}
    % Nicer default font (+ math font) than Computer Modern for most use cases
    \usepackage{mathpazo}

    % Basic figure setup, for now with no caption control since it's done
    % automatically by Pandoc (which extracts ![](path) syntax from Markdown).
    \usepackage{graphicx}
    % We will generate all images so they have a width \maxwidth. This means
    % that they will get their normal width if they fit onto the page, but
    % are scaled down if they would overflow the margins.
    \makeatletter
    \def\maxwidth{\ifdim\Gin@nat@width>\linewidth\linewidth
    \else\Gin@nat@width\fi}
    \makeatother
    \let\Oldincludegraphics\includegraphics
    % Set max figure width to be 80% of text width, for now hardcoded.
    \renewcommand{\includegraphics}[1]{\Oldincludegraphics[width=.8\maxwidth]{#1}}
    % Ensure that by default, figures have no caption (until we provide a
    % proper Figure object with a Caption API and a way to capture that
    % in the conversion process - todo).
    \usepackage{caption}
    \DeclareCaptionLabelFormat{nolabel}{}
    \captionsetup{labelformat=nolabel}

    \usepackage{adjustbox} % Used to constrain images to a maximum size 
    \usepackage{xcolor} % Allow colors to be defined
    \usepackage{enumerate} % Needed for markdown enumerations to work
    \usepackage{geometry} % Used to adjust the document margins
    \usepackage{amsmath} % Equations
    \usepackage{amssymb} % Equations
    \usepackage{textcomp} % defines textquotesingle
    % Hack from http://tex.stackexchange.com/a/47451/13684:
    \AtBeginDocument{%
        \def\PYZsq{\textquotesingle}% Upright quotes in Pygmentized code
    }
    \usepackage{upquote} % Upright quotes for verbatim code
    \usepackage{eurosym} % defines \euro
    \usepackage[mathletters]{ucs} % Extended unicode (utf-8) support
    \usepackage[utf8x]{inputenc} % Allow utf-8 characters in the tex document
    \usepackage{fancyvrb} % verbatim replacement that allows latex
    \usepackage{grffile} % extends the file name processing of package graphics 
                         % to support a larger range 
    % The hyperref package gives us a pdf with properly built
    % internal navigation ('pdf bookmarks' for the table of contents,
    % internal cross-reference links, web links for URLs, etc.)
    \usepackage{hyperref}
    \usepackage{longtable} % longtable support required by pandoc >1.10
    \usepackage{booktabs}  % table support for pandoc > 1.12.2
    \usepackage[inline]{enumitem} % IRkernel/repr support (it uses the enumerate* environment)
    \usepackage[normalem]{ulem} % ulem is needed to support strikethroughs (\sout)
                                % normalem makes italics be italics, not underlines
    

    
    
    % Colors for the hyperref package
    \definecolor{urlcolor}{rgb}{0,.145,.698}
    \definecolor{linkcolor}{rgb}{.71,0.21,0.01}
    \definecolor{citecolor}{rgb}{.12,.54,.11}

    % ANSI colors
    \definecolor{ansi-black}{HTML}{3E424D}
    \definecolor{ansi-black-intense}{HTML}{282C36}
    \definecolor{ansi-red}{HTML}{E75C58}
    \definecolor{ansi-red-intense}{HTML}{B22B31}
    \definecolor{ansi-green}{HTML}{00A250}
    \definecolor{ansi-green-intense}{HTML}{007427}
    \definecolor{ansi-yellow}{HTML}{DDB62B}
    \definecolor{ansi-yellow-intense}{HTML}{B27D12}
    \definecolor{ansi-blue}{HTML}{208FFB}
    \definecolor{ansi-blue-intense}{HTML}{0065CA}
    \definecolor{ansi-magenta}{HTML}{D160C4}
    \definecolor{ansi-magenta-intense}{HTML}{A03196}
    \definecolor{ansi-cyan}{HTML}{60C6C8}
    \definecolor{ansi-cyan-intense}{HTML}{258F8F}
    \definecolor{ansi-white}{HTML}{C5C1B4}
    \definecolor{ansi-white-intense}{HTML}{A1A6B2}

    % commands and environments needed by pandoc snippets
    % extracted from the output of `pandoc -s`
    \providecommand{\tightlist}{%
      \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}
    \DefineVerbatimEnvironment{Highlighting}{Verbatim}{commandchars=\\\{\}}
    % Add ',fontsize=\small' for more characters per line
    \newenvironment{Shaded}{}{}
    \newcommand{\KeywordTok}[1]{\textcolor[rgb]{0.00,0.44,0.13}{\textbf{{#1}}}}
    \newcommand{\DataTypeTok}[1]{\textcolor[rgb]{0.56,0.13,0.00}{{#1}}}
    \newcommand{\DecValTok}[1]{\textcolor[rgb]{0.25,0.63,0.44}{{#1}}}
    \newcommand{\BaseNTok}[1]{\textcolor[rgb]{0.25,0.63,0.44}{{#1}}}
    \newcommand{\FloatTok}[1]{\textcolor[rgb]{0.25,0.63,0.44}{{#1}}}
    \newcommand{\CharTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{{#1}}}
    \newcommand{\StringTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{{#1}}}
    \newcommand{\CommentTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textit{{#1}}}}
    \newcommand{\OtherTok}[1]{\textcolor[rgb]{0.00,0.44,0.13}{{#1}}}
    \newcommand{\AlertTok}[1]{\textcolor[rgb]{1.00,0.00,0.00}{\textbf{{#1}}}}
    \newcommand{\FunctionTok}[1]{\textcolor[rgb]{0.02,0.16,0.49}{{#1}}}
    \newcommand{\RegionMarkerTok}[1]{{#1}}
    \newcommand{\ErrorTok}[1]{\textcolor[rgb]{1.00,0.00,0.00}{\textbf{{#1}}}}
    \newcommand{\NormalTok}[1]{{#1}}
    
    % Additional commands for more recent versions of Pandoc
    \newcommand{\ConstantTok}[1]{\textcolor[rgb]{0.53,0.00,0.00}{{#1}}}
    \newcommand{\SpecialCharTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{{#1}}}
    \newcommand{\VerbatimStringTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{{#1}}}
    \newcommand{\SpecialStringTok}[1]{\textcolor[rgb]{0.73,0.40,0.53}{{#1}}}
    \newcommand{\ImportTok}[1]{{#1}}
    \newcommand{\DocumentationTok}[1]{\textcolor[rgb]{0.73,0.13,0.13}{\textit{{#1}}}}
    \newcommand{\AnnotationTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{{#1}}}}}
    \newcommand{\CommentVarTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{{#1}}}}}
    \newcommand{\VariableTok}[1]{\textcolor[rgb]{0.10,0.09,0.49}{{#1}}}
    \newcommand{\ControlFlowTok}[1]{\textcolor[rgb]{0.00,0.44,0.13}{\textbf{{#1}}}}
    \newcommand{\OperatorTok}[1]{\textcolor[rgb]{0.40,0.40,0.40}{{#1}}}
    \newcommand{\BuiltInTok}[1]{{#1}}
    \newcommand{\ExtensionTok}[1]{{#1}}
    \newcommand{\PreprocessorTok}[1]{\textcolor[rgb]{0.74,0.48,0.00}{{#1}}}
    \newcommand{\AttributeTok}[1]{\textcolor[rgb]{0.49,0.56,0.16}{{#1}}}
    \newcommand{\InformationTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{{#1}}}}}
    \newcommand{\WarningTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{{#1}}}}}
    
    
    % Define a nice break command that doesn't care if a line doesn't already
    % exist.
    \def\br{\hspace*{\fill} \\* }
    % Math Jax compatability definitions
    \def\gt{>}
    \def\lt{<}
    % Document parameters
    \title{03-Analysis}
    
    
    

    % Pygments definitions
    
\makeatletter
\def\PY@reset{\let\PY@it=\relax \let\PY@bf=\relax%
    \let\PY@ul=\relax \let\PY@tc=\relax%
    \let\PY@bc=\relax \let\PY@ff=\relax}
\def\PY@tok#1{\csname PY@tok@#1\endcsname}
\def\PY@toks#1+{\ifx\relax#1\empty\else%
    \PY@tok{#1}\expandafter\PY@toks\fi}
\def\PY@do#1{\PY@bc{\PY@tc{\PY@ul{%
    \PY@it{\PY@bf{\PY@ff{#1}}}}}}}
\def\PY#1#2{\PY@reset\PY@toks#1+\relax+\PY@do{#2}}

\expandafter\def\csname PY@tok@w\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.73,0.73}{##1}}}
\expandafter\def\csname PY@tok@c\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}
\expandafter\def\csname PY@tok@cp\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.74,0.48,0.00}{##1}}}
\expandafter\def\csname PY@tok@k\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@kp\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@kt\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.69,0.00,0.25}{##1}}}
\expandafter\def\csname PY@tok@o\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@ow\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.67,0.13,1.00}{##1}}}
\expandafter\def\csname PY@tok@nb\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@nf\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,1.00}{##1}}}
\expandafter\def\csname PY@tok@nc\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,1.00}{##1}}}
\expandafter\def\csname PY@tok@nn\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,1.00}{##1}}}
\expandafter\def\csname PY@tok@ne\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.82,0.25,0.23}{##1}}}
\expandafter\def\csname PY@tok@nv\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\expandafter\def\csname PY@tok@no\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.53,0.00,0.00}{##1}}}
\expandafter\def\csname PY@tok@nl\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.63,0.63,0.00}{##1}}}
\expandafter\def\csname PY@tok@ni\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.60,0.60,0.60}{##1}}}
\expandafter\def\csname PY@tok@na\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.49,0.56,0.16}{##1}}}
\expandafter\def\csname PY@tok@nt\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@nd\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.67,0.13,1.00}{##1}}}
\expandafter\def\csname PY@tok@s\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@sd\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@si\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.73,0.40,0.53}{##1}}}
\expandafter\def\csname PY@tok@se\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.73,0.40,0.13}{##1}}}
\expandafter\def\csname PY@tok@sr\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.40,0.53}{##1}}}
\expandafter\def\csname PY@tok@ss\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\expandafter\def\csname PY@tok@sx\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@m\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@gh\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,0.50}{##1}}}
\expandafter\def\csname PY@tok@gu\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.50,0.00,0.50}{##1}}}
\expandafter\def\csname PY@tok@gd\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.63,0.00,0.00}{##1}}}
\expandafter\def\csname PY@tok@gi\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.63,0.00}{##1}}}
\expandafter\def\csname PY@tok@gr\endcsname{\def\PY@tc##1{\textcolor[rgb]{1.00,0.00,0.00}{##1}}}
\expandafter\def\csname PY@tok@ge\endcsname{\let\PY@it=\textit}
\expandafter\def\csname PY@tok@gs\endcsname{\let\PY@bf=\textbf}
\expandafter\def\csname PY@tok@gp\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,0.50}{##1}}}
\expandafter\def\csname PY@tok@go\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.53,0.53,0.53}{##1}}}
\expandafter\def\csname PY@tok@gt\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.27,0.87}{##1}}}
\expandafter\def\csname PY@tok@err\endcsname{\def\PY@bc##1{\setlength{\fboxsep}{0pt}\fcolorbox[rgb]{1.00,0.00,0.00}{1,1,1}{\strut ##1}}}
\expandafter\def\csname PY@tok@kc\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@kd\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@kn\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@kr\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@bp\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@fm\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,1.00}{##1}}}
\expandafter\def\csname PY@tok@vc\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\expandafter\def\csname PY@tok@vg\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\expandafter\def\csname PY@tok@vi\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\expandafter\def\csname PY@tok@vm\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\expandafter\def\csname PY@tok@sa\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@sb\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@sc\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@dl\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@s2\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@sh\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@s1\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@mb\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@mf\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@mh\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@mi\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@il\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@mo\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@ch\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}
\expandafter\def\csname PY@tok@cm\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}
\expandafter\def\csname PY@tok@cpf\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}
\expandafter\def\csname PY@tok@c1\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}
\expandafter\def\csname PY@tok@cs\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}

\def\PYZbs{\char`\\}
\def\PYZus{\char`\_}
\def\PYZob{\char`\{}
\def\PYZcb{\char`\}}
\def\PYZca{\char`\^}
\def\PYZam{\char`\&}
\def\PYZlt{\char`\<}
\def\PYZgt{\char`\>}
\def\PYZsh{\char`\#}
\def\PYZpc{\char`\%}
\def\PYZdl{\char`\$}
\def\PYZhy{\char`\-}
\def\PYZsq{\char`\'}
\def\PYZdq{\char`\"}
\def\PYZti{\char`\~}
% for compatibility with earlier versions
\def\PYZat{@}
\def\PYZlb{[}
\def\PYZrb{]}
\makeatother


    % Exact colors from NB
    \definecolor{incolor}{rgb}{0.0, 0.0, 0.5}
    \definecolor{outcolor}{rgb}{0.545, 0.0, 0.0}



    
    % Prevent overflowing lines due to hard-to-break entities
    \sloppy 
    % Setup hyperref package
    \hypersetup{
      breaklinks=true,  % so long urls are correctly broken across lines
      colorlinks=true,
      urlcolor=urlcolor,
      linkcolor=linkcolor,
      citecolor=citecolor,
      }
    % Slightly bigger margins than the latex defaults
    
    \geometry{verbose,tmargin=1in,bmargin=1in,lmargin=1in,rmargin=1in}
    
    

    \begin{document}
    
    
    \maketitle
    
    

    
    \hypertarget{outline}{%
\section{Outline}\label{outline}}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Abstract
\item
  Intro
\item
  Datasets \& Algorithm Intro

  \begin{enumerate}
  \def\labelenumii{\arabic{enumii}.}
  \tightlist
  \item
    Data

    \begin{itemize}
    \tightlist
    \item
      Abalone
    \item
      Madelon
    \end{itemize}
  \item
    Algorithms

    \begin{itemize}
    \tightlist
    \item
      Decision Tree
    \item
      Boosting
    \item
      ANN
    \item
      KNN
    \item
      SVM
    \end{itemize}
  \end{enumerate}
\item
  Methodology

  \begin{itemize}
  \tightlist
  \item
    Brief overview of scripts/pipeline
  \end{itemize}
\item
  Discussion: Algorithms Analysis

  \begin{itemize}
  \tightlist
  \item
    For each algorithm:

    \begin{itemize}
    \tightlist
    \item
      Train/Test Error Rates

      \begin{itemize}
      \tightlist
      \item
        At the end if there's enough time, might want to add precision,
        recall, f1 score as metrics on best hyper param models
      \end{itemize}
    \item
      Training Time
    \item
      Learning Rate
    \item
      `Overfitting' Curves (Expressiveness)
    \item
      Hyperparameter Analysis

      \begin{itemize}
      \tightlist
      \item
        Why did these come out the best? Discuss what each parameter
        does and reasoning for why that performed best. Look at Grid
        search results if possible to see if distribution of params
        change the outcome much (i.e.~the effect of param on
        performance)
      \end{itemize}
    \end{itemize}
  \end{itemize}
\item
  Conclusion

  \begin{itemize}
  \tightlist
  \item
    Key Takeaways
  \end{itemize}
\end{enumerate}

    \hypertarget{abstract}{%
\section{Abstract}\label{abstract}}

    The world is full of problems, which means there's learning to do. If
you've got answers, I've got supervised learning techniques.
Specifically, if the problem happens to be identifying the age range of
abalones or identifying a non-linear classification with vast amounts of
noise added then the supervised learning algorithms for Decision Trees,
Boosting, Artifical Neural Networks, K Nearest Neighbours and Support
Vector Machines may be of help. For each of these datasets, and in turn
each algorithm, the accuracy of classification was tested under cross
validation over a variety of hyperparameters (learning rate,
regularization, etc.) using sci-kit learn's GridSearchCV. The resulting
hyperparameters, model performance, learning curve's and
`overfit-ability' are examined. This analysis gives way to furth insight
into both the algorithms and problems.

    \href{https://www.matec-conferences.org/articles/matecconf/pdf/2017/54/matecconf_iceesi2017_01024.pdf}{knn
cheby shev distance short explanation}

    \hypertarget{intro}{%
\section{Intro}\label{intro}}

\hypertarget{datasets}{%
\paragraph{Datasets}\label{datasets}}

Interesting analysis requires insteresting problems, and in order
illuminate the strengths, weaknesses and quirks of the examined
supervised learning algorithms two well known datasets from the UCI
machine learning repository data are examined.

    \hypertarget{abalone}{%
\paragraph{Abalone}\label{abalone}}

    \emph{Instances}: 4177

\emph{Attributes}: 10

\emph{Data Types}: Continuous (9), Categorical (1)

\emph{Classes}: 0, 1, 2 for Abelone with rings in ranges {[}0, 8{]},
{[}9,10{]}, {[}10, 28{]}

This dataset measures the continuous physical characteristics as well as
the categoric gender of shellfish abalones. The purpose of the dataset
is to classify the abalone by age, which is a function of the number of
rings inside the abalone's shell (age = rings +1.5). This is a time
consuming process which could be alleviated by predicting the age as a
function of easier to measure characteristics. The original dataset has
28 seperate classes (Rings ranging from 1-28) and in order to reduce
this the Rings were segmented into three classes, one each for rings in
ranges between 0-8, 9-10, 11+. These ranges roughly cover 1/3 of the
distribution each, with the mean number of Rings almost exactly 10. The
variance of \textasciitilde{}10 and kurtosis of 2.3 indiciate a skewed
distribution with wide dispersion. This is further evidenced by the
Rings histogram:

The distribution sharply peaks around 9-10, indicating that most
abalones may stop growing around this point or that some confounding
variable is present (such as access to food, or survival conditions such
as weather and/or harvesting). Binning down to three classes creates a
roughly balanced classification problem with \textasciitilde{}2000
instances in each class. The algorithms will be learning to discriminate
between young abalones (Class 0), average age abalones (Class 1) and
older abalones (Class 2). This will be a difficult task, as nearly all
data elements are moderately to strongly positively correlated with both
each other and the target variable (Rings/Class), as evidenced by this
correlation matrix:

The strong correlation with the classes should aid in discriminating
between classes 0 and 2, but the multicolinearity of the features may
make it difficult to distinguish the difference between the large range
of ages (Class 0, Class 2) and the average aged abalones (Class 1). In
other words, the algorithms will need to distinguish which physical
traits seperate the young abalones from average age ablones and the old
abalones from average age abalones. All from variables that are strongly
correlated with one another with similar distributions. The differing
weight metrics are all very similar with a large positive skew in their
distribution, while length and diameter are negatively skewed. Height
has minimal variance indicating it may be uninformative.

    \hypertarget{madelon}{%
\paragraph{Madelon}\label{madelon}}

    \emph{Instances}: 5000

\emph{Attributes}: 440

\emph{Data Types}: Continuous (440)

\emph{Classes}: 0, 1

The MADELON dataset is an artifical dataset created in 2003 for the NIPs
conference as part of a feature selection challenge. The target class
comes from a group of 32 clusters on the vertices of a five dimensional
hypercube. Those points were randomly assigned a class (either 1 or -1).
Additionally the five dimensions were transformed by linear combinations
to form fifteen more features. To complicated the problem 480 features
of random noise were added to the dataset.

Of particular interest here is that the Madelon dataset presents a
highly non-linear problem where the signal-to-noise ratio is very low.
1\% of the features are truly useful (the 5 dimensions) while 15 (3\%)
are superflouus albeit still informative. This leaves 96\% as completely
useless to learn from. To alleviate some of the imbalance in
signal-to-noise ratio, sklearn's feature selection method
SelectFromModel in tandem with a RandomForestClassifier was implemented.
The feature selection was repeated four times with a threshold set to
`median', i.e.~any feature deemed to be in the lower half of feature
importance is dropped. In other words, the more important half of the
features were kept with this repeated four times leaving 31 features for
the algorithm to learn from. In the best case scenario, this would leave
the 20 informative features and 11 noise features.

In addition the noise issues, the non-linearity of the problem presents
an interesting challenge to the learning algorithms. Algorithms without
the expressivenes to describe non-linear patterns, e.g.~a linear SVM,
may struggle on the dataset while others, e.g.~an SVM with RBF kernel,
may have improved performance.

    \hypertarget{algorithms-methodology}{%
\subsection{Algorithms \& Methodology}\label{algorithms-methodology}}

    All algorithms were implemented via the python machine learning package
sci-kit learn.

Fo each algorithm, the learner was five fold cross validation trained
using balanced accuracy as the performance metric across a variety of
hyperparameters. The best parameters were stored, with the best
performing classifier then trained on varying amounts of the data with
its performance and wall clock time recorded to illustrate its learning
curve and computation cost. The variance or `overfit-ablility' of the
iterative learners (ANN, Boosting, SVMs) was tested by measuring the
train and test accuracy across an increasing number of iterations using
hyper parameters with high expressiveness (i.e.~regularization
parameters set to very low values)

\hypertarget{artificial-neural-network}{%
\paragraph{Artificial Neural Network}\label{artificial-neural-network}}

\emph{Hyper Parameters Searched: Activation Function, Learning Rate,
Hidden Layer Size}

\hypertarget{boosting}{%
\paragraph{Boosting}\label{boosting}}

\emph{Hyper Parameters Searched: Number of Estimators, Learning Rate (of
base estimator Decision Tree)}

\hypertarget{decision-tree}{%
\paragraph{Decision Tree}\label{decision-tree}}

\emph{Hyper Parameters Searched: Splitting Criteria, Learning Rate, Node
Count}

\hypertarget{k-nearest-neighbours}{%
\paragraph{K Nearest Neighbours}\label{k-nearest-neighbours}}

\emph{Hyper Parameters Searched: Distance Metric, Number of Neighbours,
Weighting of Neighbours Method}

\hypertarget{svm}{%
\paragraph{SVM}\label{svm}}

\emph{Hyper Parameters Searched: Kernel (Linear, RBF), Learning Rate,
Number of Iterations}

    \hypertarget{algorithm-analysis}{%
\subsection{Algorithm Analysis}\label{algorithm-analysis}}

    \begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{4}
\tightlist
\item
  Discussion: Algorithms Analysis

  \begin{itemize}
  \tightlist
  \item
    For each algorithm:

    \begin{itemize}
    \tightlist
    \item
      Train/Test Error Rates

      \begin{itemize}
      \tightlist
      \item
        At the end if there's enough time, might want to add precision,
        recall, f1 score as metrics on best hyper param models
      \end{itemize}
    \item
      Training Time
    \item
      Learning Rate
    \item
      `Overfitting' Curves (Expressiveness)
    \item
      Hyperparameter Analysis

      \begin{itemize}
      \tightlist
      \item
        Why did these come out the best? Discuss what each parameter
        does and reasoning for why that performed best. Look at Grid
        search results if possible to see if distribution of params
        change the outcome much (i.e.~the effect of param on
        performance)
      \end{itemize}
    \end{itemize}
  \end{itemize}
\end{enumerate}

\hypertarget{artificial-neural-network}{%
\paragraph{Artificial Neural Network}\label{artificial-neural-network}}

    \hypertarget{alg}{%
\paragraph{Alg}\label{alg}}

\emph{Abalone Best 5 Fold CV Score:}

\emph{Hyperparameters Selected:} - - -

\emph{Madelon Best 5 Fold CV Score:}

\emph{Hyperparameters Selected:} - - -

\emph{Learning Curve}

\emph{Timing Curve}

\emph{`Overfitting' Curve}

    \hypertarget{artificial-neural-network}{%
\paragraph{Artificial Neural Network}\label{artificial-neural-network}}

\hypertarget{section}{%
\paragraph{}\label{section}}

\emph{Abalone Best 5 Fold CV Score:}

\textbf{.6211}

\emph{Hyperparameters Selected}

\begin{itemize}
\item
  Activation Function: Relu
\item
  Learning Rate: .1
\item
  Hidden Layer Size: (20, 20, 20)
\end{itemize}

\emph{Madelon Best 5 Fold CV Score:}

\textbf{0.7628}

\emph{Hyperparameters Selected:}

\begin{itemize}
\item
  Activation Function: Relu
\item
  Learning Rate: .0001
\item
  Hidden Layer Size: (62, 62, 62)
\end{itemize}

\emph{Learning Curve}

\emph{Timing Curve}

\emph{`Overfitting' Curve}

    \hypertarget{decision-tree}{%
\paragraph{Decision Tree}\label{decision-tree}}

\emph{Abalone Best 5 Fold CV Score:} \textbf{0.6182}

\emph{Hyperparameters Selected:}

\begin{verbatim}
- Learning Rate: 0.01
- Class Weigh: Balanced
- Splitting Criteria: Gini
\end{verbatim}

\emph{Madelon Best 5 Fold CV Score:}

\emph{Hyperparameters Selected:} \textbf{0.8102}

\begin{verbatim}
- Learning Rate: 0
- Class Weigh: Balanced
- Splitting Criteria: Gini
\end{verbatim}

\emph{Learning Curve}

\emph{Timing Curve}

\emph{`Overfitting' Curve}

    \hypertarget{boosting}{%
\paragraph{Boosting}\label{boosting}}

\emph{Abalone Best 5 Fold CV Score:} \textbf{0.6282}

\emph{Hyperparameters Selected:}

\begin{verbatim}
- Base Decision Tree Learning Rate
-
-
\end{verbatim}

\emph{Madelon Best 5 Fold CV Score:}

\emph{Hyperparameters Selected:} - - -

\emph{Learning Curve}

\emph{Timing Curve}

\emph{`Overfitting' Curve}


    % Add a bibliography block to the postdoc
    
    
    
    \end{document}
