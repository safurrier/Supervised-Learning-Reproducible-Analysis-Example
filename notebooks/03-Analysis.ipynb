{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment\n",
    "\n",
    "The Problems Given to You\n",
    "You should implement five learning algorithms. They are for:\n",
    "\n",
    "Decision trees with some form of pruning\n",
    "Neural networks\n",
    "Boosting\n",
    "Support Vector Machines\n",
    "k-nearest neighbors\n",
    "Each algorithm is described in detail in your textbook, the handouts, and all over the web. In fact, instead of implementing the algorithms yourself, you may (and by may I mean should) use software packages that you find elsewhere; however, if you do so you should provide proper attribution. Also, you will note that you have to do some fiddling to get good results, graphs and such, so even if you use another's package, you may need to be able to modify it in various ways.\n",
    "\n",
    "Decision Trees. For the decision tree, you should implement or steal a decision tree algorithm (and by \"implement or steal\" I mean \"steal\"). Be sure to use some form of pruning. You are not required to use information gain (for example, there is something called the GINI index that is sometimes used) to split attributes, but you should describe whatever it is that you do use.\n",
    "\n",
    "Neural Networks. For the neural network you should implement or steal your favorite kind of network and training algorithm. You may use networks of nodes with as many layers as you like and any activation function you see fit.\n",
    "\n",
    "Boosting. Implement or steal a boosted version of your decision trees. As before, you will want to use some form of pruning, but presumably because you're using boosting you can afford to be much more aggressive about your pruning.\n",
    "\n",
    "Support Vector Machines. You should implement (for sufficently loose definitions of implement including \"download\") SVMs. This should be done in such a way that you can swap out kernel functions. I'd like to see at least two.\n",
    "\n",
    "k-Nearest Neighbors. You should \"implement\" (the quotes mean I don't mean it: steal the code) kNN. Use different values of k.\n",
    "\n",
    "Testing. In addition to implementing (wink), the algorithms described above, you should design two interesting classification problems. For the purposes of this assignment, a classification problem is just a set of training examples and a set of test examples. I don't care where you get the data. You can download some, take some from your own research, or make some up on your own. Be careful about the data you choose, though. You'll have to explain why they are interesting, use them in later assignments, and come to really care about them.\n",
    "\n",
    "What to Turn In\n",
    "You must submit a tar or zip file named yourgtaccount.{zip,tar,tar.gz} that contains a single folder or directory named yourgtaccount (where gtaccount is probably something like gtg7577b or maybe ci16). That directory in turn contains:\n",
    "\n",
    "a file named README.txt containing instructions for running your code (see note below)\n",
    "your code (see note below)\n",
    "a file named yourgtaccount-analysis.pdf containing your writeup\n",
    "any supporting files you need, such as your training and test sets (see note below).\n",
    "Note below: if the data are way, way, too huge for submitting, see if you can arrange for an URL. This also goes for code, too. Submitting all of Weka isn't necessary, for example, because I can get it myself; however, you should at least submit any files you found necessary to change and enough support and explanation so we could reproduce your results if we really wanted to do so. In any case, include all the information in README.txt\n",
    "\n",
    "The file yourgtaccount-analysis.pdf should contain:\n",
    "\n",
    "a description of your classification problems, and why you feel that they are interesting. Think hard about this. To be at all interesting the problems should be non-trivial on the one hand, but capable of admitting comparisons and analysis of the various algorithms on the other. \n",
    "the training and testing error rates you obtained running the various learning algorithms on your problems. At the very least you should include graphs that show performance on both training and test data as a function of training size (note that this implies that you need to design a classification problem that has more than a trivial amount of data) and--for the algorithms that are iterative--training times/iterations. Both of these kinds of graphs are referred to as learning curves, BTW.\n",
    "analyses of your results. Why did you get the results you did? Compare and contrast the different algorithms. What sort of changes might you make to each of those algorithms to improve performance? How fast were they in terms of wall clock time? Iterations? Would cross validation help (and if it would, why didn't you implement it?)? How much performance was due to the problems you chose? How about the values you chose for learning rates, stopping criteria, pruning methods, and so forth (and why doesn't your analysis show results for the different values you chose?)? Which algorithm performed best? How do you define best? Be creative and think of as many questions you can, and as many answers as you can.\n",
    "For the sanity of your graders, please keep your analysis as short as possible while still covering the requirements of the assignment: to facilitate this sanity, analysis writeup is limited to 12 pages.\n",
    "\n",
    "Grading Criteria\n",
    "You are being graded on your analysis more than anything else. Roughly speaking, implementing everything and getting it to run is worth maybe 0% of the grade on this assignment (I know you don't believe me, but in fact, steal the code; I not only don't care, I am encouraging you to use one of the many packages available both from the resources page and on the web). Of course, analysis without proof of working code makes the analysis suspect.\n",
    "\n",
    "The key thing is that your explanations should be both thorough and concise. Imagine you are writing a paper for the major conference in your field the year you will be graduating and you need to impress all those folks who will be deciding whether to interview you later. You don't want them to think you're shallow do you? Or that you're incapable of coming up with interesting classification problems, right? And you surely don't want them to think that you make up for a lack of content by blathering on about irrelevant aspects of your work? Of course not.\n",
    "\n",
    "Finally, I'd like to point out that I am very particular about the format of the assignments. Follow the directions carefully. Failure to turn in files with the proper naming scheme, or anything else that makes the graders' lives unduly hard is simply going to lead to an ignored assignment. I am remarkably inflexible about this. Also, there will be no late assignments accepted, so start now. Have fun. One day you'll look back on this and smile. There may be tears, but they will be tears of joy.\n",
    "\n",
    "When your assignment is graded, you will receive feedback explaining your errors (and your successes!) in some level of detail. This feedback is for your benefit, both on this assignment and for future assignments. It is considered a part of your learning goals to internalize this feedback. \n",
    "\n",
    "If you are convinced that your grade is in error in light of the feedback, you may request a regrade within a week of the grade and feedback being returned to you. A regrade request is only valid if it includes an explanation of where the grader made an error. Send a private Piazza post to only the head TA. In the Summary add “[Request] Regrade <whichever assignment>”. In the Details add sufficient explanation as to why you think the grader made a mistake. Be concrete and specific. We will not consider requests that do not follow these directions.\n",
    "\n",
    "It is important to note that because we consider your ability to internalize feedback a learning goal, we also assess it. This ability is considered 10% of each assignment. We default to assigning you full credit. If you request a regrade and do not receive at least 5 points as a result of the request, you will lose those 10 points.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Outline\n",
    "\n",
    "1. Abstract\n",
    "2. Intro\n",
    "3. Datasets & Algorithm Intro\n",
    "    1. Data\n",
    "        - Abalone\n",
    "        - Madelon\n",
    "    2. Algorithms\n",
    "        - Decision Tree\n",
    "        - Boosting\n",
    "        - ANN\n",
    "        - KNN\n",
    "        - SVM\n",
    "4. Methodology\n",
    "    * Brief overview of scripts/pipeline\n",
    "5. Discussion: Algorithms Analysis\n",
    "    * For each algorithm: \n",
    "        * Train/Test Error Rates\n",
    "            * At the end if there's enough time, might want to add\n",
    "            precision, recall, f1 score as metrics on best hyper param models\n",
    "        * Training Time\n",
    "        * Learning Rate\n",
    "        * 'Overfitting' Curves (Expressiveness)\n",
    "        * Hyperparameter Analysis\n",
    "            * Why did these come out the best? \n",
    "            Discuss what each parameter does and reasoning for why that performed best. \n",
    "            Look at Grid search results if possible to see if distribution of params change \n",
    "            the outcome much (i.e. the effect of param on performance)\n",
    "        \n",
    "\n",
    "6. Conclusion\n",
    "    * Key Takeaways\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Abstract"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The world is full of problems, which means there's learning to do. If you've got answers, I've got supervised learning techniques. Specifically, if the problem happens to be identifying the age range of abalones or identifying a non-linear classification with vast amounts of noise added, the supervised learning algorithms for Decision Trees, Boosting, Artifical Neural Networks, K Nearest Neighbours and Support Vector Machines may be of help. For each of these datasets, and in turn each algorithm, the accuracy of classification was tested under cross validation over a variety of hyperparameters (learning rate, regularization, etc.) using sci-kit learn's GridSearchCV. The resulting hyperparameters, model performance, learning curve's and 'overfit-ability' are examined. This analysis gives way to furth insight into both the algorithms and problems."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[knn cheby shev distance short explanation](https://www.matec-conferences.org/articles/matecconf/pdf/2017/54/matecconf_iceesi2017_01024.pdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "ml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
